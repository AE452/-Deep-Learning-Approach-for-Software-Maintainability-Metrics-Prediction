{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cf85fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6add9001",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "#importing libraries\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcaae2",
   "metadata": {},
   "source": [
    "## Reading Csv\n",
    "There are three files on which it must be performed the Feature Subset Selection, I am sorry but you should run and change manually the name, I tried to automize all the process but some libraries used for feature selection are not so stable and you will see that sometimes you have to restart the kernel and run only that specific part. I will leave the name the three files used below. For the file-level csv the process is automatic as you can see at the end of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96075042",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neo4j-Unified_class.csv\n",
    "#mct-1.7b1-Unified_class.csv\n",
    "#Elasticsearch-0.90.11-Unified_class.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0854388",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv('../data/mct-1.7b1-Unified_class.csv')\n",
    "a.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7f6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = ['ID', 'Type', 'Name', 'LongName', 'Parent', 'Component', 'Path', 'Line',\n",
    "       'Column', 'EndLine', 'EndColumn'] #personal data that is ignored in the lstm modeling\n",
    "fea = ['CC', 'CCL', 'CCO', 'CI', 'CLC',\n",
    "       'CLLC', 'LDC', 'LLDC', 'LCOM5', 'NL', 'NLE', 'WMC', 'CBO', 'CBOI',\n",
    "       'NII', 'NOI', 'RFC', 'AD', 'CD', 'CLOC', 'DLOC', 'PDA', 'PUA', 'TCD',\n",
    "       'TCLOC', 'DIT', 'NOA', 'NOD', 'NOP', 'LLOC', 'LOC', 'NA', 'NG',\n",
    "       'NLA', 'NLG', 'NLM', 'NLPA', 'NLPM', 'NLS', 'NM', 'NOS', 'NPA', 'NPM',\n",
    "       'NS', 'TLLOC', 'TLOC', 'TNA', 'TNG', 'TNLA', 'TNLG', 'TNLM', 'TNLPA',\n",
    "       'TNLPM', 'TNLS', 'TNM', 'TNOS', 'TNPA', 'TNPM', 'TNS', 'bug'] #features on which FSS is performed\n",
    "target = ['NOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a59907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just see how many missing values there are in the df\n",
    "(a.isna().sum()/len(a)).sort_values(ascending = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5256d50",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2dc488",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a[fea].describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cbb29f",
   "metadata": {},
   "source": [
    "## Normalize Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e382326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data)) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678e41fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.NOC = NormalizeData(a.NOC)\n",
    "tgt = (a['NOC']>0)*1\n",
    "tgt.value_counts() #tgt represent the dependent variable (target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd765db3",
   "metadata": {},
   "source": [
    "## Clean and save the Original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517ec9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "a[fea]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa99112",
   "metadata": {},
   "outputs": [],
   "source": [
    "totcols = [*fea, *target]\n",
    "totdf = a[totcols]\n",
    "totdf.to_csv('./Datasets/OriginalDataset_MCT_class.csv', sep =';', index = False)\n",
    "pd.read_csv('./Datasets/OriginalDataset_MCT_class.csv', sep =';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ddb48",
   "metadata": {},
   "source": [
    "## Features Subset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e68312",
   "metadata": {},
   "source": [
    "Feature selection is a process where you automatically select those features in your data that contribute most to the prediction variable or output in which you are interested.\n",
    "\n",
    "Having irrelevant features in your data can decrease the accuracy of many models, especially linear algorithms like linear and logistic regression.\n",
    "\n",
    "Three benefits of performing feature selection before modeling your data are:\n",
    "\n",
    "- Reduces Overfitting: Less redundant data means less opportunity to make decisions based on noise.\n",
    "- Improves Accuracy: Less misleading data means modeling accuracy improves.\n",
    "- Reduces Training Time: Less data means that algorithms train faster.\n",
    "\n",
    "We have to choose the optimal subset using the following algorithms:\n",
    "- Linear Correlation\n",
    "- Rank Correlation\n",
    "- One R\n",
    "- Relief (Filter Method)\n",
    "- Consistency "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc04a95",
   "metadata": {},
   "source": [
    "### Linear Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db341631",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Linear Correlation using Pearson Correlation\n",
    "cor = a[fea].corr()\n",
    "cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0257b067",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(cor, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ad1130",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = np.full((cor.shape[0],), True, dtype=bool)\n",
    "for i in range(cor.shape[0]):\n",
    "    for j in range(i+1, cor.shape[0]):\n",
    "        if cor.iloc[i,j] >= 0.55:\n",
    "            if cols[j]:\n",
    "                cols[j] = False\n",
    "selected_columns_lcorr = a[fea].columns[cols]\n",
    "data = a[selected_columns_lcorr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c80c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_columns_lcorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ba7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_lcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e0ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['NOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20549570",
   "metadata": {},
   "outputs": [],
   "source": [
    "totcols = [*selected_columns_lcorr, *target]\n",
    "totdf = a[totcols]\n",
    "totdf.to_csv('./Datasets/LinearCorr_MCT_class.csv', sep =';', index = False)\n",
    "pd.read_csv('./Datasets/LinearCorr_MCT_class.csv', sep =';') #just to check the current format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d494327",
   "metadata": {},
   "source": [
    "### Rank Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fb44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rank Correlation using Kendall Correlation: \n",
    "kcorr = a[fea].corr(method = 'kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc678bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(kcorr, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29282b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_r = np.full((kcorr.shape[0],), True, dtype=bool)\n",
    "for i in range(kcorr.shape[0]):\n",
    "    for j in range(i+1, kcorr.shape[0]):\n",
    "        if abs(kcorr.iloc[i,j]) >= 0.55: ## \n",
    "            if cols_r[j]:\n",
    "                cols_r[j] = False\n",
    "selected_columns_rankcorr = a[fea].columns[cols_r]\n",
    "data = a[selected_columns_rankcorr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914bd781",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_rankcorr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642399d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(selected_columns_rankcorr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123494a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "totcols = [*selected_columns_rankcorr, *target]\n",
    "totdf = a[totcols]\n",
    "totdf.to_csv('./Datasets/RankCorr_NEO4j_class.csv', sep =';', index = False)\n",
    "pd.read_csv('./Datasets/RankCorr_NEO4j_class.csv', sep =';') #just to check the current format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88399efa",
   "metadata": {},
   "source": [
    "### OneR\n",
    "\"OneR\" stands for One Rule (by Robert Holte [1]), which is a classic algorithm for supervised learning. Note that this algorithm is not known for its good prediction performance; thus, it is rather recommended for teaching purposes and for lower-bound performance baselines in real-world applications.\n",
    "\n",
    "The name \"OneRule\" can be a bit misleading, because it is technically about \"one feature\" and not about \"one rule.\" I.e., OneR returns a feature for which one or more decision rules are defined. Essentially, as a simple classifier, it finds exactly one feature (and one or more feature values for that feature) to classify data instances.\n",
    "\n",
    "The basic procedure is as follows:\n",
    "\n",
    "For each feature among all features (columns) in the dataset:\n",
    "For each feature value for the given feature:\n",
    "- Obtain the training examples with that feature value.\n",
    "- Obtain the class labels (and class label counts) corresponding to the training examples identified in the previous step.\n",
    "\n",
    "- Regard the class label with the highest frequency (count) as the majority class.\n",
    "- Record the number of errors as the number of training examples that have the given feature value but are not the majority class.\n",
    "- Compute the error of the feature by summing the errors for all possible feature values for that feature.\n",
    "- Return the best feature, which is defined as the feature with the lowest error.\n",
    "\n",
    "source : http://rasbt.github.io/mlxtend/user_guide/classifier/OneRClassifier/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd231284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboost for feature importance on a classification problem\n",
    "from sklearn.datasets import make_classification\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc5868",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns_rankcorr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c88e1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = XGBClassifier()\n",
    "# fit the model\n",
    "model.fit(a[fea].values, a['NOC'].values)\n",
    "# get importance\n",
    "importance = model.feature_importances_\n",
    "# summarize feature importance\n",
    "for i,v in enumerate(importance):\n",
    "    print('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "pyplot.figure(figsize = (19,9))\n",
    "pyplot.bar([x for x in range(len(importance))], importance)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56922f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bb = {}\n",
    "\n",
    "for i in range(0,len(importance)):\n",
    "    bb[a[fea].columns[i]] = (importance[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d40ec1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sort_orders = sorted(bb.items(), key=lambda x: x[1], reverse=True)\n",
    "oneR_fea = []\n",
    "for i in sort_orders:\n",
    "    print(i[0])\n",
    "    oneR_fea.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f470b85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sort_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb6e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneR_fea = oneR_fea[0:15]\n",
    "oneR_fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b0a816",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['NOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc0316",
   "metadata": {},
   "outputs": [],
   "source": [
    "totcols = [*oneR_fea, *target]\n",
    "len(totcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e816c",
   "metadata": {},
   "outputs": [],
   "source": [
    "totcols = [*oneR_fea, *target]\n",
    "totdf = a[totcols]\n",
    "totdf.to_csv('./Datasets/OneR_NEO4j_class.csv', sep =';', index = False)\n",
    "pd.read_csv('./Datasets/OneR_NEO4j_class.csv', sep =';') #just to check the current format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dafaa6",
   "metadata": {},
   "source": [
    "### Relief\n",
    "Relief is an algorithm developed by Kira and Rendell in 1992 that takes a filter-method approach to feature selection that is notably sensitive to feature interactions.\n",
    "It was originally designed for application to binary classification problems with discrete or numerical features. Relief calculates a feature score for each feature which can then be applied to rank and select top scoring features for feature selection. Alternatively, these scores may be applied as feature weights to guide downstream modeling. Relief feature scoring is based on the identification of feature value differences between nearest neighbor instance pairs.\n",
    "https://www.sciencedirect.com/science/article/pii/S1532046418301400\n",
    "\n",
    "\n",
    "Critics = Relief (Kira & Rendell, 1992) does not use a feature set evaluation function, and it does not even perform a search in the feature set space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a628d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sklearn_relief"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819e1e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sklearn_relief as sr\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7083dcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "relief_vars = []\n",
    "r = sr.RReliefF(n_features = 15)\n",
    "ff = r.fit_transform(a[fea].values, a['NOC'].values)\n",
    "relief_df = pd.DataFrame(ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ef0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "relief_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb85e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "relief_vars=[]\n",
    "for k in range(relief_df.shape[1]):\n",
    "    ovo = a.eq(relief_df.iloc[:, k], axis=0).sum(axis=0)/len(a)\n",
    "    c = ovo[ovo==1].idxmax() \n",
    "    relief_vars.append(c) \n",
    "    #if you got an error about the empty sequence, restart the kernel and run this method as first, it will work\n",
    "    #or simpy rerun "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52abd2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(relief_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dc3f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['NOC']\n",
    "totcols = [*relief_vars, *target]\n",
    "totdf = a[totcols]\n",
    "totdf.to_csv('./Datasets/Relief_NEO4J_class.csv', sep =';', index = False)\n",
    "pd.read_csv('./Datasets/Relief_NEO4J_class.csv', sep =';') #just to check the current format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cf9edb",
   "metadata": {},
   "source": [
    "### Consistency\n",
    "\n",
    "Fuzzy Rough Feature Selection (FRFS) greedily selects features that induce the greatest increase in the size of the positive region, until it matches the size of the positive region with all features, or until the required number of features is selected.\n",
    "\n",
    "The positive region is defined as the union of the lower approximations of the decision classes in X. Its size is the sum of its membership values.\n",
    "\n",
    "The similarity relation equation M1 for a given subset of attributes B is obtained by aggregating with a t-norm the per-attribute similarities equation M2 associated with the attributes a in B. These are in turn defined, for any equation M3, as the complement of the difference between the attribute values equation M4 and equation M5 after rescaling by the sample standard deviation equation M6 (1).\n",
    "\n",
    "equation M7\n",
    "Paper: https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.645.151&rep=rep1&type=pdf\n",
    "\n",
    "Library: https://github.com/oulenz/fuzzy-rough-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b288a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fuzzy-rough-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa32feb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from frlearn.base import select_class\n",
    "from frlearn.feature_preprocessors import FRFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdcb611",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c162d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p = a.head(3000) #reduce df's dimension because it is computationally expensive\n",
    "X_orig = df_p[fea].values\n",
    "y = df_p['NOC'].values\n",
    "preprocessor = FRFS(n_features=15)\n",
    "model = preprocessor(X_orig, y)\n",
    "X = model(X_orig)\n",
    "cons_df = pd.DataFrame(X)\n",
    "cons_vars=[]\n",
    "for k in range(cons_df.shape[1]):\n",
    "    ovo = df_p[fea].eq(cons_df.iloc[:, k], axis=0).sum(axis=0)/len(df_p[fea])\n",
    "    c = ovo[ovo==1].idxmax()\n",
    "    cons_vars.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763130f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cons_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef095c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['NOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4690f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "totcols = [*cons_vars, *target]\n",
    "totdf = a[totcols]\n",
    "totdf.to_csv('./Datasets/Consistency_NEO4j_class.csv', sep =';', index = False)\n",
    "pd.read_csv('./Datasets/Consistency_NEO4j_class.csv', sep =';') #just to check the current format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7cb313",
   "metadata": {},
   "source": [
    "## Processing the file-level csv\n",
    "For this kind of file that presents less than 10 variables, even if some are repeated, doesn't make sense run a Feature Subset Selection. So I clean and fix the dataset, in order to use it directly in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f8c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeData(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data)) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63480b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir('../data/'):\n",
    "    if file.endswith('file.csv'):\n",
    "        print('Processing the file:  ',file)\n",
    "        df=pd.read_csv('../data/'+file)\n",
    "        fea = ['McCC', 'CLOC', 'PDA', 'PUA',\n",
    "               'LLOC', 'LOC', 'Number of previous fixes',\n",
    "               'Number of developer commits', 'Number of committers',\n",
    "               'bug']\n",
    "        target = ['Number of previous modifications']\n",
    "        # Target normalization\n",
    "        df['Number of previous modifications'] = NormalizeData(df['Number of previous modifications'])\n",
    "        totcols = [*fea, *target]\n",
    "        totdf = df[totcols]\n",
    "        print('DONE')\n",
    "        totdf.to_csv('./Datasets/OriginalDataset_' + file, sep =';', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc37d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a check if it works\n",
    "pd.read_csv('./Datasets/OriginalDataset_Elasticsearch-0.90.11-Unified_file.csv', sep =';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
